{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LLM Classifier Interpretability Analysis\n",
        "\n",
        "This notebook provides interactive exploration of the trained LLM classifier through:\n",
        "\n",
        "1. **Logistic Regression Coefficient Analysis** - Understanding which hidden dimensions drive predictions\n",
        "2. **Logit Lens Risk Trajectories** - Visualizing how risk evolves across patient sequences\n",
        "3. **Spurious Correlation Detection** - Identifying non-clinical patterns that may be driving predictions\n",
        "\n",
        "## Model Architecture Recap\n",
        "\n",
        "```\n",
        "Input Tokens → Qwen3-8B + LoRA → Hidden States (B, T, 4096) → Last Token → Linear(4096, 2) → Cancer/Control\n",
        "```\n",
        "\n",
        "The classifier weights `W ∈ R^(2×4096)` directly encode how each hidden dimension contributes to the prediction.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import display, HTML\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add project root to path\n",
        "project_root = os.path.abspath(os.path.join(os.getcwd(), '../../..'))\n",
        "if project_root not in sys.path:\n",
        "    sys.path.insert(0, project_root)\n",
        "\n",
        "print(f\"Project root: {project_root}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import interpretability module\n",
        "from src.pipelines.text_based.interpretability import (\n",
        "    load_classifier_for_analysis,\n",
        "    extract_lr_weights,\n",
        "    compute_risk_trajectory,\n",
        "    get_top_contributing_tokens,\n",
        "    visualize_weight_distribution,\n",
        "    visualize_risk_trajectory,\n",
        "    visualize_top_risk_tokens,\n",
        "    detect_spurious_patterns,\n",
        ")\n",
        "from src.data.unified_dataset import UnifiedEHRDataset\n",
        "from src.training.utils import seed_all\n",
        "\n",
        "seed_all(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Set the paths to your config and checkpoint:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CONFIGURATION - Update these paths for your setup\n",
        "# ============================================================\n",
        "\n",
        "CONFIG_PATH = \"src/pipelines/text_based/configs/llm_classify_pretrained_cls_lora.yaml\"\n",
        "CHECKPOINT_PATH = \"/data/scratch/qc25022/pancreas/experiments/lora-6-month-logistic-raw/checkpoint-7856\"\n",
        "OUTPUT_DIR = \"./interpretability_results_notebook\"\n",
        "\n",
        "# Device\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Config: {CONFIG_PATH}\")\n",
        "print(f\"Checkpoint: {CHECKPOINT_PATH}\")\n",
        "print(f\"Output: {OUTPUT_DIR}\")\n",
        "print(f\"Device: {DEVICE}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Model and Tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the trained classifier\n",
        "model, tokenizer, config = load_classifier_for_analysis(\n",
        "    config_path=CONFIG_PATH,\n",
        "    checkpoint_path=CHECKPOINT_PATH,\n",
        "    device=DEVICE\n",
        ")\n",
        "\n",
        "print(f\"\\nModel loaded successfully!\")\n",
        "print(f\"Classifier shape: {model.classifier.weight.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Logistic Regression Weight Analysis\n",
        "\n",
        "The classifier is a simple linear layer: `logits = W @ hidden_state + b`\n",
        "\n",
        "For binary classification:\n",
        "- `W[0]` = weights for control class\n",
        "- `W[1]` = weights for cancer class\n",
        "- `W[1] - W[0]` = log-odds direction (positive = more cancer-like)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract and analyze weights\n",
        "weight_analysis = extract_lr_weights(model, top_k=50)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CLASSIFIER WEIGHT STATISTICS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "stats = weight_analysis.statistics\n",
        "for key, value in stats.items():\n",
        "    print(f\"  {key:20s}: {value:.6f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize weight distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Histogram\n",
        "ax1 = axes[0]\n",
        "ax1.hist(weight_analysis.diff_weights, bins=100, edgecolor='black', alpha=0.7, color='steelblue')\n",
        "ax1.axvline(x=0, color='red', linestyle='--', linewidth=2, label='Zero')\n",
        "ax1.axvline(x=stats['mean'], color='green', linestyle='--', linewidth=2, label=f\"Mean={stats['mean']:.4f}\")\n",
        "ax1.set_xlabel('Weight (Cancer - Control)', fontsize=12)\n",
        "ax1.set_ylabel('Frequency', fontsize=12)\n",
        "ax1.set_title('Distribution of Log-Odds Weights', fontsize=14)\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Sorted weights\n",
        "ax2 = axes[1]\n",
        "sorted_weights = np.sort(weight_analysis.diff_weights)\n",
        "ax2.plot(sorted_weights, linewidth=0.5)\n",
        "ax2.axhline(y=0, color='red', linestyle='--', linewidth=1)\n",
        "ax2.fill_between(range(len(sorted_weights)), 0, sorted_weights,\n",
        "                 where=(sorted_weights > 0), color='crimson', alpha=0.3)\n",
        "ax2.fill_between(range(len(sorted_weights)), 0, sorted_weights,\n",
        "                 where=(sorted_weights < 0), color='steelblue', alpha=0.3)\n",
        "ax2.set_xlabel('Dimension (sorted)', fontsize=12)\n",
        "ax2.set_ylabel('Weight Value', fontsize=12)\n",
        "ax2.set_title('Sorted Weights: Cancer vs Control', fontsize=14)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, 'weight_distribution_interactive.png'), dpi=150)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Top dimensions\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TOP 25 DIMENSIONS PUSHING TOWARD CANCER (Positive Weights)\")\n",
        "print(\"=\"*70)\n",
        "for i, (dim, weight) in enumerate(weight_analysis.top_positive_dims[:25], 1):\n",
        "    bar = '█' * int(abs(weight) * 20)\n",
        "    print(f\"{i:3d}. Dim {dim:4d}: {weight:+.5f} {bar}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TOP 25 DIMENSIONS PUSHING TOWARD CONTROL (Negative Weights)\")\n",
        "print(\"=\"*70)\n",
        "for i, (dim, weight) in enumerate(weight_analysis.top_negative_dims[:25], 1):\n",
        "    bar = '█' * int(abs(weight) * 20)\n",
        "    print(f\"{i:3d}. Dim {dim:4d}: {weight:+.5f} {bar}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Dataset for Trajectory Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load validation dataset\n",
        "data_config = config['data']\n",
        "\n",
        "dataset_args = {\n",
        "    \"data_dir\": data_config[\"data_dir\"],\n",
        "    \"vocab_file\": data_config[\"vocab_filepath\"],\n",
        "    \"labels_file\": data_config[\"labels_filepath\"],\n",
        "    \"medical_lookup_file\": data_config[\"medical_lookup_filepath\"],\n",
        "    \"lab_lookup_file\": data_config[\"lab_lookup_filepath\"],\n",
        "    \"region_lookup_file\": data_config[\"region_lookup_filepath\"],\n",
        "    \"time_lookup_file\": data_config[\"time_lookup_filepath\"],\n",
        "    \"format\": 'text',\n",
        "    \"cutoff_months\": data_config.get(\"cutoff_months\", 1),\n",
        "    \"max_sequence_length\": None,\n",
        "    \"tokenizer\": None,\n",
        "    \"data_type\": data_config.get('data_type', 'raw')\n",
        "}\n",
        "\n",
        "val_dataset = UnifiedEHRDataset(split=\"tuning\", **dataset_args)\n",
        "print(f\"Loaded {len(val_dataset)} validation samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find cancer and control patients\n",
        "cancer_indices = []\n",
        "control_indices = []\n",
        "\n",
        "for i in range(min(len(val_dataset), 500)):\n",
        "    sample = val_dataset[i]\n",
        "    if sample is not None:\n",
        "        label = sample['label'].item() if torch.is_tensor(sample['label']) else sample['label']\n",
        "        if label > 0:\n",
        "            cancer_indices.append(i)\n",
        "        else:\n",
        "            control_indices.append(i)\n",
        "\n",
        "print(f\"Found {len(cancer_indices)} cancer patients\")\n",
        "print(f\"Found {len(control_indices)} control patients\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Risk Trajectory Analysis (Logit Lens)\n",
        "\n",
        "The Logit Lens applies the classifier weights to hidden states at **every position**, not just the last token. This shows how the model's \"opinion\" evolves as it reads through the patient's history.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_patient(patient_idx, dataset, model, tokenizer, weight_analysis, output_dir):\n",
        "    \"\"\"Analyze and visualize a single patient.\"\"\"\n",
        "    sample = dataset[patient_idx]\n",
        "    text = sample['text']\n",
        "    label = sample['label'].item() if torch.is_tensor(sample['label']) else sample['label']\n",
        "    label_str = \"CANCER\" if label > 0 else \"CONTROL\"\n",
        "    \n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"PATIENT {patient_idx} - {label_str}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    # Compute trajectory\n",
        "    trajectory = compute_risk_trajectory(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        text=text,\n",
        "        weight_analysis=weight_analysis,\n",
        "        true_label=label,\n",
        "        max_length=12000,\n",
        "        device=DEVICE\n",
        "    )\n",
        "    \n",
        "    print(f\"\\nSequence Length: {len(trajectory.tokens)} tokens\")\n",
        "    print(f\"Final P(Cancer): {trajectory.final_prediction:.4f}\")\n",
        "    print(f\"Final Log-Odds:  {trajectory.risk_scores[-1]:.4f}\")\n",
        "    print(f\"Max Risk Score:  {np.max(trajectory.risk_scores):.4f}\")\n",
        "    print(f\"Min Risk Score:  {np.min(trajectory.risk_scores):.4f}\")\n",
        "    \n",
        "    # Plot trajectory\n",
        "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(16, 8), sharex=True)\n",
        "    \n",
        "    subsample = max(1, len(trajectory.tokens) // 500)\n",
        "    positions = trajectory.positions[::subsample]\n",
        "    risk_scores = trajectory.risk_scores[::subsample]\n",
        "    probabilities = trajectory.probabilities[::subsample]\n",
        "    \n",
        "    # Log-odds plot\n",
        "    ax1.plot(positions, risk_scores, color='crimson', linewidth=1, alpha=0.8)\n",
        "    ax1.fill_between(positions, 0, risk_scores,\n",
        "                     where=(risk_scores > 0), color='crimson', alpha=0.3, label='Cancer risk')\n",
        "    ax1.fill_between(positions, 0, risk_scores,\n",
        "                     where=(risk_scores <= 0), color='steelblue', alpha=0.3, label='Control risk')\n",
        "    ax1.axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
        "    ax1.set_ylabel('Log-Odds (Cancer vs Control)', fontsize=12)\n",
        "    ax1.set_title(f'Risk Trajectory - Patient {patient_idx} ({label_str})', fontsize=14)\n",
        "    ax1.legend(loc='upper left')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Probability plot\n",
        "    ax2.plot(positions, probabilities, color='purple', linewidth=1, alpha=0.8)\n",
        "    ax2.fill_between(positions, 0.5, probabilities,\n",
        "                     where=(probabilities > 0.5), color='crimson', alpha=0.3)\n",
        "    ax2.fill_between(positions, 0.5, probabilities,\n",
        "                     where=(probabilities <= 0.5), color='steelblue', alpha=0.3)\n",
        "    ax2.axhline(y=0.5, color='black', linestyle='--', linewidth=1)\n",
        "    ax2.set_xlabel('Token Position', fontsize=12)\n",
        "    ax2.set_ylabel('P(Cancer)', fontsize=12)\n",
        "    ax2.set_ylim(0, 1)\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_dir, f'trajectory_patient_{patient_idx}.png'), dpi=150)\n",
        "    plt.show()\n",
        "    \n",
        "    return trajectory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze a cancer patient\n",
        "if cancer_indices:\n",
        "    cancer_patient_idx = cancer_indices[0]\n",
        "    cancer_trajectory = analyze_patient(\n",
        "        cancer_patient_idx, val_dataset, model, tokenizer, weight_analysis, OUTPUT_DIR\n",
        "    )\n",
        "else:\n",
        "    print(\"No cancer patients found in the scanned range.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze a control patient\n",
        "if control_indices:\n",
        "    control_patient_idx = control_indices[0]\n",
        "    control_trajectory = analyze_patient(\n",
        "        control_patient_idx, val_dataset, model, tokenizer, weight_analysis, OUTPUT_DIR\n",
        "    )\n",
        "else:\n",
        "    print(\"No control patients found in the scanned range.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. High-Risk Token Analysis\n",
        "\n",
        "Which tokens appear at positions where the risk score is highest? This can reveal:\n",
        "- **Clinical signals**: Symptoms, diagnoses, lab values that indicate cancer risk\n",
        "- **Spurious patterns**: Administrative tokens, dates, doctor names that shouldn't be predictive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_high_risk_tokens(trajectory, top_k=20):\n",
        "    \"\"\"Display tokens at high-risk positions with context.\"\"\"\n",
        "    top_tokens = get_top_contributing_tokens(trajectory, top_k=top_k, context_window=10)\n",
        "    \n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"TOP {top_k} HIGH-RISK POSITIONS\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    for i, info in enumerate(top_tokens, 1):\n",
        "        print(f\"\\n{i}. Position {info['position']}: Score={info['risk_score']:.4f}, P={info['probability']:.4f}\")\n",
        "        print(f\"   Token: '{info['token']}'\")\n",
        "        print(f\"   Context: ...{info['context'][:100]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show high-risk tokens for the cancer patient\n",
        "if 'cancer_trajectory' in dir():\n",
        "    show_high_risk_tokens(cancer_trajectory, top_k=15)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize token risk distribution\n",
        "if 'cancer_trajectory' in dir():\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "    \n",
        "    # Get top and bottom tokens\n",
        "    sorted_idx = np.argsort(cancer_trajectory.risk_scores)\n",
        "    top_cancer = sorted_idx[-20:][::-1]\n",
        "    top_control = sorted_idx[:20]\n",
        "    \n",
        "    # Top cancer tokens\n",
        "    tokens_cancer = [f\"[{i}] {cancer_trajectory.tokens[i][:12]}\" for i in top_cancer]\n",
        "    scores_cancer = [cancer_trajectory.risk_scores[i] for i in top_cancer]\n",
        "    ax1.barh(tokens_cancer[::-1], scores_cancer[::-1], color='crimson', edgecolor='black', alpha=0.7)\n",
        "    ax1.set_xlabel('Log-Odds Score', fontsize=12)\n",
        "    ax1.set_title('Top 20 Tokens -> Cancer', fontsize=14)\n",
        "    ax1.grid(True, alpha=0.3, axis='x')\n",
        "    \n",
        "    # Top control tokens\n",
        "    tokens_control = [f\"[{i}] {cancer_trajectory.tokens[i][:12]}\" for i in top_control]\n",
        "    scores_control = [cancer_trajectory.risk_scores[i] for i in top_control]\n",
        "    ax2.barh(tokens_control[::-1], scores_control[::-1], color='steelblue', edgecolor='black', alpha=0.7)\n",
        "    ax2.set_xlabel('Log-Odds Score', fontsize=12)\n",
        "    ax2.set_title('Top 20 Tokens -> Control', fontsize=14)\n",
        "    ax2.grid(True, alpha=0.3, axis='x')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(OUTPUT_DIR, 'high_risk_tokens.png'), dpi=150)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Interactive Patient Explorer\n",
        "\n",
        "Use this section to explore specific patients interactively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Choose a patient index to analyze\n",
        "PATIENT_INDEX = cancer_indices[1] if len(cancer_indices) > 1 else (control_indices[0] if control_indices else 0)\n",
        "\n",
        "trajectory = analyze_patient(\n",
        "    PATIENT_INDEX, val_dataset, model, tokenizer, weight_analysis, OUTPUT_DIR\n",
        ")\n",
        "show_high_risk_tokens(trajectory, top_k=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Spurious Correlation Detection\n",
        "\n",
        "Analyze multiple patients to detect patterns that might indicate spurious correlations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect trajectories from multiple patients\n",
        "import random\n",
        "\n",
        "all_trajectories = []\n",
        "sample_cancer = random.sample(cancer_indices, min(5, len(cancer_indices)))\n",
        "sample_control = random.sample(control_indices, min(5, len(control_indices)))\n",
        "\n",
        "print(\"Analyzing cancer patients...\")\n",
        "for idx in sample_cancer:\n",
        "    sample = val_dataset[idx]\n",
        "    text = sample['text']\n",
        "    label = sample['label'].item() if torch.is_tensor(sample['label']) else sample['label']\n",
        "    \n",
        "    traj = compute_risk_trajectory(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        text=text,\n",
        "        weight_analysis=weight_analysis,\n",
        "        true_label=label,\n",
        "        max_length=12000,\n",
        "        device=DEVICE\n",
        "    )\n",
        "    all_trajectories.append(traj)\n",
        "    print(f\"  Patient {idx}: P(Cancer)={traj.final_prediction:.3f}\")\n",
        "\n",
        "print(\"\\nAnalyzing control patients...\")\n",
        "for idx in sample_control:\n",
        "    sample = val_dataset[idx]\n",
        "    text = sample['text']\n",
        "    label = sample['label'].item() if torch.is_tensor(sample['label']) else sample['label']\n",
        "    \n",
        "    traj = compute_risk_trajectory(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        text=text,\n",
        "        weight_analysis=weight_analysis,\n",
        "        true_label=label,\n",
        "        max_length=12000,\n",
        "        device=DEVICE\n",
        "    )\n",
        "    all_trajectories.append(traj)\n",
        "    print(f\"  Patient {idx}: P(Cancer)={traj.final_prediction:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run spurious detection\n",
        "spurious_results = detect_spurious_patterns(all_trajectories, tokenizer, top_k=50)\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"SPURIOUS CORRELATION ANALYSIS\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "print(f\"\\nAnalyzed {len(all_trajectories)} patient trajectories\")\n",
        "\n",
        "print(\"\\nMost Common High-Risk Tokens:\")\n",
        "print(\"-\" * 50)\n",
        "for i, (token, count) in enumerate(spurious_results['high_risk_tokens'][:20], 1):\n",
        "    token_display = token.replace('\\n', '\\\\n')[:30]\n",
        "    print(f\"  {i:2d}. '{token_display}': {count} occurrences\")\n",
        "\n",
        "if spurious_results['warnings']:\n",
        "    print(f\"\\nWARNINGS ({spurious_results['num_warnings']} potential spurious patterns):\")\n",
        "    print(\"-\" * 50)\n",
        "    for warning in spurious_results['warnings']:\n",
        "        print(f\"  - {warning['warning']}\")\n",
        "else:\n",
        "    print(\"\\nNo obvious spurious patterns detected in high-risk tokens\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Save Analysis Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.pipelines.text_based.interpretability import save_analysis_results\n",
        "\n",
        "# Save weight analysis\n",
        "save_analysis_results(weight_analysis, OUTPUT_DIR)\n",
        "\n",
        "# Save spurious analysis\n",
        "import json\n",
        "spurious_path = os.path.join(OUTPUT_DIR, \"spurious_analysis.json\")\n",
        "with open(spurious_path, 'w') as f:\n",
        "    serializable_results = {\n",
        "        \"high_risk_tokens\": [(str(t), c) for t, c in spurious_results['high_risk_tokens']],\n",
        "        \"low_risk_tokens\": [(str(t), c) for t, c in spurious_results['low_risk_tokens']],\n",
        "        \"warnings\": spurious_results['warnings'],\n",
        "        \"num_warnings\": spurious_results['num_warnings']\n",
        "    }\n",
        "    json.dump(serializable_results, f, indent=2)\n",
        "\n",
        "print(f\"\\nResults saved to: {OUTPUT_DIR}\")\n",
        "print(\"\\nGenerated files:\")\n",
        "for f in os.listdir(OUTPUT_DIR):\n",
        "    print(f\"  - {f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook provided:\n",
        "\n",
        "1. **Weight Analysis**: Extracted and visualized the logistic regression coefficients, showing which hidden dimensions push toward cancer vs control predictions.\n",
        "\n",
        "2. **Risk Trajectories**: Applied the Logit Lens to visualize how the model's \"opinion\" evolves as it reads patient histories.\n",
        "\n",
        "3. **Token Attribution**: Identified tokens at high-risk positions to understand what clinical (or spurious) signals drive predictions.\n",
        "\n",
        "4. **Spurious Detection**: Analyzed patterns across multiple patients to flag potential non-clinical biases.\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- **Integrated Gradients**: For more precise token-level attribution, consider implementing Integrated Gradients using the Captum library.\n",
        "- **Attention Analysis**: Visualize attention patterns to understand token interactions.\n",
        "- **Dimension Probing**: Investigate what specific hidden dimensions encode (e.g., by correlating with known clinical features).\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
