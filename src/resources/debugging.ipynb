{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "618486e4",
   "metadata": {},
   "source": [
    "## Patient split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc7853e8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "753e5e55",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train ∩ tuning: 31\n",
      "[29862151292, 85275550336, 119515850170, 154114250099, 233566951376, 236591550323, 252079650966, 308943650489, 357161451323, 440126050081, 580271950622, 663653650209, 671073751005, 883921650892, 919730151187, 999841951213, 1098339850231, 1107969350995, 1107984051061, 1175490050051] ...\n",
      "held_out ∩ train: 31\n",
      "[46769150755, 198123350377, 296742651040, 307420250806, 352566250206, 479875450346, 512631750810, 531489850080, 565650551076, 575435950411, 577684650964, 607911150269, 653984850621, 659473751128, 766021751155, 826132950659, 859353250945, 862857051077, 924505250131, 1024391150422] ...\n",
      "held_out ∩ tuning: 6\n",
      "[699772750809, 1123054650125, 1153887650945, 1286978550840, 1575689850232, 1612772050559] ...\n"
     ]
    }
   ],
   "source": [
    "DATA_ROOT = \"/data/scratch/qc25022/pancreas/tokenised_data_word_level/cprd_upgi\"\n",
    "SPLITS = [\"train\", \"tuning\", \"held_out\"]  # add any extra splits you used\n",
    "\n",
    "def collect_subjects(split):\n",
    "    split_dir = os.path.join(DATA_ROOT, split)\n",
    "    subjects = set()\n",
    "    for fname in os.listdir(split_dir):\n",
    "        if not fname.endswith(\".pkl\"):\n",
    "            continue\n",
    "        with open(os.path.join(split_dir, fname), \"rb\") as f:\n",
    "            for record in pickle.load(f):\n",
    "                subjects.add(record[\"subject_id\"])\n",
    "    return subjects\n",
    "\n",
    "split_subjects = {split: collect_subjects(split) for split in SPLITS}\n",
    "\n",
    "# pairwise intersection report\n",
    "for a in SPLITS:\n",
    "    for b in SPLITS:\n",
    "        if a >= b:\n",
    "            continue\n",
    "        overlap = split_subjects[a] & split_subjects[b]\n",
    "        print(f\"{a} ∩ {b}: {len(overlap)}\")\n",
    "        if overlap:\n",
    "            print(sorted(list(overlap))[:20], \"...\")  # sample IDs if debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33de656e",
   "metadata": {},
   "source": [
    "## Trajectory Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91e87da7-369f-43fa-b4bb-9d383e8b53ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/home/qc25022/CancEHR-Training/src/resources\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m repo_root = notebook_path.parents[\u001b[32m1\u001b[39m]      \u001b[38;5;66;03m# move up from src/resources to repo root\u001b[39;00m\n\u001b[32m      7\u001b[39m sys.path.insert(\u001b[32m0\u001b[39m, \u001b[38;5;28mstr\u001b[39m(repo_root))\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01munified_dataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m UnifiedEHRDataset\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/CancEHR-Training/src/data/unified_dataset.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pathlib\n",
    "import sys\n",
    "notebook_path = pathlib.Path().resolve()  # this is the directory Jupyter started in\n",
    "print(notebook_path)\n",
    "repo_root = notebook_path.parents[1]      # move up from src/resources to repo root\n",
    "sys.path.insert(0, str(repo_root))\n",
    "from src.data.unified_dataset import UnifiedEHRDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a39d6d8d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'UnifiedEHRDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m summary(lengths_chars), summary(lengths_tokens)\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtuning\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mheld_out\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     char_stats, token_stats = \u001b[43mdescribe_lengths\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcutoff\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m12\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m char stats: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchar_stats\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     29\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m token stats: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoken_stats\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mdescribe_lengths\u001b[39m\u001b[34m(split, cutoff)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdescribe_lengths\u001b[39m(split, cutoff):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     dataset = \u001b[43mUnifiedEHRDataset\u001b[49m(\n\u001b[32m      3\u001b[39m         data_dir=DATA_ROOT,\n\u001b[32m      4\u001b[39m         vocab_file=VOCAB,\n\u001b[32m      5\u001b[39m         labels_file=LABELS,\n\u001b[32m      6\u001b[39m         medical_lookup_file=MEDICAL,\n\u001b[32m      7\u001b[39m         lab_lookup_file=LAB,\n\u001b[32m      8\u001b[39m         region_lookup_file=REGION,\n\u001b[32m      9\u001b[39m         time_lookup_file=TIME,\n\u001b[32m     10\u001b[39m         cutoff_months=cutoff,\n\u001b[32m     11\u001b[39m         \u001b[38;5;28mformat\u001b[39m=\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     12\u001b[39m         split=split,\n\u001b[32m     13\u001b[39m         max_sequence_length=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     14\u001b[39m     )\n\u001b[32m     15\u001b[39m     lengths_chars = []\n\u001b[32m     16\u001b[39m     lengths_tokens = []\n",
      "\u001b[31mNameError\u001b[39m: name 'UnifiedEHRDataset' is not defined"
     ]
    }
   ],
   "source": [
    "def describe_lengths(split, cutoff):\n",
    "    dataset = UnifiedEHRDataset(\n",
    "        data_dir=DATA_ROOT,\n",
    "        vocab_file=VOCAB,\n",
    "        labels_file=LABELS,\n",
    "        medical_lookup_file=MEDICAL,\n",
    "        lab_lookup_file=LAB,\n",
    "        region_lookup_file=REGION,\n",
    "        time_lookup_file=TIME,\n",
    "        cutoff_months=cutoff,\n",
    "        format=\"text\",\n",
    "        split=split,\n",
    "        max_sequence_length=None,\n",
    "    )\n",
    "    lengths_chars = []\n",
    "    lengths_tokens = []\n",
    "    for item in dataset:\n",
    "        if item is None:\n",
    "            continue\n",
    "        text = item[\"text\"]\n",
    "        lengths_chars.append(len(text))\n",
    "        lengths_tokens.append(len(text.split()))  # crude word count; swap with tokenizer.encode if desired\n",
    "    summary = lambda arr: dict(count=len(arr), mean=np.mean(arr), p95=np.percentile(arr, 95), max=max(arr))\n",
    "    return summary(lengths_chars), summary(lengths_tokens)\n",
    "\n",
    "for split in [\"train\", \"tuning\", \"held_out\"]:\n",
    "    char_stats, token_stats = describe_lengths(split, cutoff=12)\n",
    "    print(f\"{split} char stats: {char_stats}\")\n",
    "    print(f\"{split} token stats: {token_stats}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e25626-7218-4e72-8f90-0345e804a351",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
